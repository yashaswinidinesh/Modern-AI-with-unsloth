# Modern AI with Unsloth â€” CMPE255 Colabs

A compact set of Colab notebooks showcasing multiple adaptation strategies for large language models (LLMs) and vision-language models with **Unsloth**, **Hugging Face Transformers**, **Datasets**, and **TRL (Reinforcement Learning for Transformers)**. Each notebook is self-contained and designed to run on Google Colab GPUs.

> 

---

## ğŸ“š Project Index (Colab â†”ï¸ Video)

### 1) CMPE255_Mistral_Finetuning_v1 â€” **Full-Parameter Fine-Tuning (SFT)**
- Colab: [Open in Colab](ADD_COLAB_LINK_HERE)
- Video: [YouTube](https://youtu.be/YkelSl8ZRFw)

### 2) CMPE255_Mistral_Lora_Finetunning_colab_2_Yashaswini_Dinesh â€” **PEFT with LoRA (Low-Rank Adaptation) + SFT**
- Colab: [Open in Colab](ADD_COLAB_LINK_HERE)
- Video: [YouTube](https://youtu.be/v2Hld-1gN2Y)

### 3) CMPE255_Mistral_Lora_Finetuning_Reinforcement_Sentimental_colab_3 â€” **LoRA + RL (PPO) for Positive Sentiment**
- Colab: [Open in Colab](ADD_COLAB_LINK_HERE)
- Video: [YouTube](https://youtu.be/aDRBJRug2M0)

### 4) CMPE255_Colab_4_Gemma3_Vision_GRPO â€” **Gemma 3 Vision + GRPO (Generalized Reinforcement Policy Optimization)**
- Colab: [Open in Colab](ADD_COLAB_LINK_HERE)
- Video: [YouTube](https://youtu.be/dRzSiNLbgLU)

### 5) CMPE255_unsloth_cpt_colab5 â€” **CPT/DAP (Continued/Domain-Adaptive Pretraining)**
- Colab: [Open in Colab](ADD_COLAB_LINK_HERE)
- Video: [YouTube](https://youtu.be/j5LzfcMUAl8)

---

## ğŸ§ª What Each Notebook Demonstrates

| Notebook | Model | Method | Objective | Alignment | Best For |
|---|---|---|---|---|---|
| v1 Full FT | Mistralâ€‘7B | **Full-parameter SFT** | Instruction following | Optional DPO/PPO later | Maximum capacity changes |
| LoRA SFT | Mistralâ€‘7B | **LoRA (PEFT) + SFT** | Instruction following | Optional DPO/PPO later | Lowâ€‘VRAM, fast iteration |
| LoRA + RL | Mistralâ€‘7B | **LoRA + PPO** | Task + **positive sentiment** reward | PPO with KL control | Controllable tone/style |
| Gemma3 Vision + GRPO | Gemma 3 Vision | **GRPO** | Visionâ€‘language tasks | GRPO | Multimodal alignment |
| CPT/DAP | Mistralâ€‘7B | **Continued pretraining** | CLM on raw domain text | Optional SFT + DPO/PPO | Domain knowledge/style |

> **SFT = Supervised Fine-Tuning** Â· **PEFT = Parameterâ€‘Efficient Fineâ€‘Tuning** Â· **LoRA = Lowâ€‘Rank Adaptation** Â· **PPO = Proximal Policy Optimization** Â· **DPO = Direct Preference Optimization** Â· **GRPO = Generalized Reinforcement Policy Optimization** Â· **CPT/DAP = Continued/Domainâ€‘Adaptive Pretraining** Â· **CLM = Causal Language Modeling**

---

## âš™ï¸ Environment (Colab-friendly)

- Recommended GPU: T4 / L4 / A100 (BF16 preferred if available).
- Python packages (typical): `unsloth`, `transformers`, `datasets`, `trl`, `accelerate`, `bitsandbytes`, `peft`.

```bash
pip install -U unsloth transformers datasets trl accelerate bitsandbytes peft
```

> On first run, Colab may prompt you to restart the runtime after installing packages.

---

## ğŸ—‚ï¸ Data & Formatting

- **Instruction datasets** as `(prompt, response)` pairs.
- Convert to **two-turn chat** (user â†’ assistant) using `tokenizer.apply_chat_template(...)`.
- Set `eos_token_id` and `pad_token_id` explicitly for Mistral/Gemma families.

**Tip:** Use the **same chat template** for training and inference (`add_generation_prompt=True`) to improve stability.

---

## ğŸƒ Quick Recipes

### Full-Parameter SFT (v1)
- `FastLanguageModel.from_pretrained(..., full_finetune=True)`  
- **BF16**, gradient accumulation, gradient checkpointing (if needed)  
- `SFTTrainer` with linear LR schedule, warmup, AdamW (8â€‘bit optional)

### LoRA SFT (colab_2)
- `get_peft_model(...)` to inject LoRA into `q_proj/k_proj/v_proj/o_proj`  
- Tune **rank (r)**, **alpha**, **dropout** for VRAM/quality tradeâ€‘off  
- `SFTTrainer` with short `max_steps` for quick demos

### LoRA + RL (colab_3)
- Start from the SFT LoRA checkpoint  
- Define a **sentiment reward** (classifier or heuristic)  
- Use **TRL PPOTrainer** with a **KL penalty** to avoid drift

### CPT / DAP (colab_5)
- CLM on **raw domain text** with **packing** for long, efficient sequences  
- AdamW, warmup, cosine/linear scheduler; optional **LoRAâ€‘CPT** for low VRAM

### Gemma 3 Vision + GRPO (colab_4)
- Visionâ€‘language dataset (image + text)  
- Optimize with **GRPO** for stable alignment on multimodal prompts

---

## ğŸ” Inference

1. Reuse the **same chat template** and set `add_generation_prompt=True`.  
2. Tokenize with **attention mask**.  
3. `model.generate(max_new_tokens=..., temperature=..., top_p=...)`.  
4. Decode and postâ€‘process (strip special tokens).

---

## âœ… Evaluation (suggested)

- Hold out a small **validation set** that mirrors your target tasks.  
- Track **loss/perplexity** (CPT) and task metrics (SFT/RL).  
- Do **A/B** comparisons: base vs. adapted model for the same prompts.

---

## ğŸ§¯ Troubleshooting

- **CUDA OOM:** lower batch size, enable 8â€‘bit optimizer, reduce context length, or use LoRA.  
- **No BF16:** fall back to FP16; consider gradient checkpointing.  
- **Weird formatting:** ensure consistent chat templates train â†”ï¸ infer.  
- **Unstable PPO:** adjust reward scaling and **KL coefficient**.

---

## ğŸ“ Add Your Colab Links Here

Paste your Colab URLs above where it says `ADD_COLAB_LINK_HERE`. A common pattern is:
```
https://colab.research.google.com/github/<user-or-org>/<repo>/blob/main/<notebook>.ipynb
```

---

## ğŸ“ License & Attribution

- Models and weights follow their original licenses (Mistral, Gemma, etc.).  
- This repo uses **Unsloth** and **Hugging Face** libraries; please cite/acknowledge accordingly.
