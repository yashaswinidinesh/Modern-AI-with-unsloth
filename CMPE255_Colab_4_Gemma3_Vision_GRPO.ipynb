{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qV0kfsIvCx8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "os.environ[\"UNSLOTH_VLLM_STANDBY\"] = \"1\" # [NEW] Extra 30% context lengths!\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install or uv pip install\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    pass # For Colab / Kaggle, we need extra instructions hidden below \\/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\", # Llama 3.2 vision support\n",
        "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\", # Can fit in a 80GB card!\n",
        "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",              # Pixtral fits in 16GB!\n",
        "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",         # Pixtral base model\n",
        "\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",          # Qwen2 VL support\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
        "\n",
        "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",      # Any Llava variant works!\n",
        "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"unsloth/gemma-3-4b-it\",\n",
        "    load_in_4bit = True, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # False if not finetuning vision layers\n",
        "    finetune_language_layers   = True, # False if not finetuning language layers\n",
        "    finetune_attention_modules = True, # False if not finetuning attention layers\n",
        "    finetune_mlp_modules       = True, # False if not finetuning MLP layers\n",
        "\n",
        "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    use_gradient_checkpointing = \"unsloth\", # Reduces memory usage\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from trl import GRPOConfig, GRPOTrainer\n",
        "import torch\n",
        "\n",
        "dataset = load_dataset(\"AI4Math/MathVista\",split=\"testmini\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL5AQL4PCHGq"
      },
      "source": [
        "Let us see what our data looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oTioAh62CHGq"
      },
      "outputs": [],
      "source": [
        "dataset[\"decoded_image\"][5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkUjslUkCHGq"
      },
      "outputs": [],
      "source": [
        "dataset[\"question\"][5], dataset[\"answer\"][5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Fd9RmocCHGq"
      },
      "source": [
        "The image of our data is a part of the math problem.\n",
        "\n",
        "To make the rewarding easy later on, let us filter only the numeric answer so that we can create reward function that gives score if reward is float or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ka5ZqAuFCHGq"
      },
      "outputs": [],
      "source": [
        "def is_numeric_answer(example):\n",
        "  try:\n",
        "    float(example[\"answer\"])\n",
        "    return True\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "dataset = dataset.filter(is_numeric_answer) #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y5duJcZCHGq"
      },
      "source": [
        "We also resize the images to be 512 by 512 pixels to make the images managable in context length. We also convert them to RGB so they are compatible with TRL's trainer!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X53sB1EhCHGq"
      },
      "outputs": [],
      "source": [
        "# Filter have big images\n",
        "def resize_images(example):\n",
        "    image = example[\"decoded_image\"]\n",
        "    image = image.resize((512,512))\n",
        "    example[\"decoded_image\"] = image\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(resize_images)\n",
        "\n",
        "def convert_to_rgb(example):\n",
        "    image = example[\"decoded_image\"]\n",
        "    if image.mode != \"RGB\":\n",
        "        image = image.convert(\"RGB\")\n",
        "    example[\"decoded_image\"] = image\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(convert_to_rgb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1GrxnxSCHGq"
      },
      "outputs": [],
      "source": [
        "# Define the delimiter variables for clarity and easy modification\n",
        "REASONING_START = \"<REASONING>\"\n",
        "REASONING_END = \"</REASONING>\"\n",
        "SOLUTION_START = \"<SOLUTION>\"\n",
        "SOLUTION_END = \"</SOLUTION>\"\n",
        "\n",
        "def make_conversation(example):\n",
        "    # Define placeholder constants if they are not defined globally\n",
        "\n",
        "    # The user's text prompt\n",
        "    text_content = (\n",
        "        f\"{example['question']}, provide your reasoning between {REASONING_START} and {REASONING_END} \"\n",
        "        f\"and then your final answer between {SOLUTION_START} and (put a float here) {SOLUTION_END}\"\n",
        "    )\n",
        "\n",
        "    # Construct the prompt in the desired multi-modal format\n",
        "    prompt = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\"},  # Placeholder for the image\n",
        "                {\"type\": \"text\", \"text\": text_content},  # The text part of the prompt\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    # The actual image data is kept separate for the processor\n",
        "    return {\"prompt\": prompt, \"image\": example[\"decoded_image\"], \"answer\": example[\"answer\"]}\n",
        "\n",
        "train_dataset = dataset.map(make_conversation)\n",
        "\n",
        "#We reformatting dataset like this because decoded_images are the actual images\n",
        "#The \"image\": example[\"decoded_image\"] does not properly format the dataset correctly\n",
        "\n",
        "# 1. Remove the original 'image' column\n",
        "train_dataset = train_dataset.remove_columns(\"image\")\n",
        "\n",
        "# 2. Rename 'decoded_image' to 'image'\n",
        "train_dataset = train_dataset.rename_column(\"decoded_image\", \"image\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYS9rjqvCHGr"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(\n",
        "    lambda example: {\n",
        "        \"prompt\": tokenizer.apply_chat_template(\n",
        "            example[\"prompt\"],\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISIv0E0sCHGr"
      },
      "outputs": [],
      "source": [
        "# Reward functions\n",
        "def formatting_reward_func(completions,**kwargs):\n",
        "    import re\n",
        "    thinking_pattern = f'{REASONING_START}(.*?){REASONING_END}'\n",
        "    answer_pattern = f'{SOLUTION_START}(.*?){SOLUTION_END}'\n",
        "\n",
        "    scores=[]\n",
        "    for completion in completions :\n",
        "      score=0\n",
        "      thinking_matches = re.findall(thinking_pattern, completion, re.DOTALL)\n",
        "      answer_matches = re.findall(answer_pattern, completion, re.DOTALL)\n",
        "      if len(thinking_matches) == 1 :\n",
        "        score +=1.0\n",
        "      if len(answer_matches) == 1 :\n",
        "        score +=1.0\n",
        "      scores.append(score)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def correctness_reward_func(prompts, completions, answer, **kwargs) -> list[float]:\n",
        "    import re\n",
        "\n",
        "    answer_pattern = f'{SOLUTION_START}(.*?){SOLUTION_END}'\n",
        "\n",
        "    responses = [re.findall(answer_pattern, completion, re.DOTALL) for completion in completions]\n",
        "    q = prompts[0]\n",
        "\n",
        "    print('-'*20, f\"Question:\\n{q}\", f\"\\nAnswer:\\n{answer[0]}\", f\"\\nResponse:{completions[0]}\")\n",
        "    return [2.0 if len(r)==1 and a == r[0].replace('\\n','') else 0.0 for r, a in zip(responses, answer)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASl_f3bqCHGr"
      },
      "outputs": [],
      "source": [
        "train_dataset[0][\"prompt\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSMu5MbUCHGr"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "\n",
        "Now set up GRPO Trainer and all configurations!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4m61cpgpCHGr"
      },
      "outputs": [],
      "source": [
        "from trl import GRPOConfig, GRPOTrainer\n",
        "\n",
        "training_args = GRPOConfig(\n",
        "    learning_rate = 5e-6,\n",
        "    adam_beta1 = 0.9,\n",
        "    adam_beta2 = 0.99,\n",
        "    weight_decay = 0.1,\n",
        "    warmup_ratio = 0.1,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    optim = \"adamw_8bit\",\n",
        "    logging_steps = 1,\n",
        "    per_device_train_batch_size = 1,\n",
        "    gradient_accumulation_steps = 2, # Increase to 4 for smoother training\n",
        "    num_generations = 4, # Decrease if out of memory\n",
        "    max_prompt_length = 1024,\n",
        "    max_completion_length = 1024,\n",
        "    #num_train_epochs = 2, # Set to 1 for a full training run\n",
        "    importance_sampling_level = \"sequence\",\n",
        "    mask_truncated_completions=False,\n",
        "    loss_type='dr_grpo',\n",
        "    max_steps = 60,\n",
        "    save_steps = 60,\n",
        "    max_grad_norm = 0.1,\n",
        "    report_to = \"none\", # Can use Weights & Biases\n",
        "    output_dir = \"outputs\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dB_J_7vJCHGr"
      },
      "source": [
        "\n",
        "| Step | Training Loss | reward    | reward_std | completion_length | kl       |\n",
        "|------|---------------|-----------|------------|-------------------|----------|\n",
        "| 1    | 0.000000      | 0.125000  | 0.000000   | 200.000000        | 0.000000 |\n",
        "| 2    | 0.000000      | 0.072375  | 0.248112   | 200.000000        | 0.000000 |\n",
        "| 3    | 0.000000      | -0.079000 | 0.163776   | 182.500000        | 0.000005 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44VvQbLRCHGr"
      },
      "outputs": [],
      "source": [
        "trainer = GRPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    # Pass the processor to handle multimodal inputs\n",
        "    processing_class=tokenizer,\n",
        "    reward_funcs=[formatting_reward_func, correctness_reward_func],\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "\n",
        "\n",
        "We'll use the best hyperparameters for inference on Gemma: `top_p=0.95`, `top_k=64`, and `temperature=1.0`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "image = dataset[100][\"decoded_image\"]\n",
        "instruction = (\n",
        "    f\"{dataset[100][\"question\"]}, provide your reasoning between {REASONING_START} and {REASONING_END} \"\n",
        "    f\"and then your final answer between {SOLUTION_START} and (put a float here) {SOLUTION_END}\"\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": instruction}],\n",
        "    }\n",
        "]\n",
        "\n",
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "result = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                        use_cache=True, temperature = 1.0, top_p = 0.95, top_k = 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\")  # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# processor.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastVisionModel\n",
        "\n",
        "    model, processor = FastVisionModel.from_pretrained(\n",
        "        model_name=\"lora_model\",  # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit=True,  # Set to False for 16bit LoRA\n",
        "    )\n",
        "    FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "sample = dataset[1]\n",
        "image = sample[\"decoded_image\"].convert(\"RGB\")\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\n",
        "                \"type\": \"text\",\n",
        "                \"text\": (\n",
        "                    f\"{sample[\"question\"]}, provide your reasoning between {REASONING_START} and {REASONING_END} \"\n",
        "                    f\"and then your final answer between {SOLUTION_START} and (put a float here) {SOLUTION_END}\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"type\": \"image\",\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "]\n",
        "input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "result = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128,\n",
        "                        use_cache=True, temperature = 1.0, top_p = 0.95, top_k = 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Select ONLY 1 to save! (Both not needed!)\n",
        "\n",
        "# Save locally to 16bit\n",
        "if False: model.save_pretrained_merged(\"unsloth_finetune\", tokenizer,)\n",
        "\n",
        "# To export and save to your Hugging Face account\n",
        "if False: model.push_to_hub_merged(\"YOUR_USERNAME/unsloth_finetune\", tokenizer, token = \"PUT_HERE\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}