{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDV34W70B3Re"
      },
      "outputs": [],
      "source": [
        "\n",
        "%pip -q install --upgrade pip\n",
        "!pip install \"unsloth>=0.3.0\" \"torch>=2.0.0\" \"transformers>=4.36.0\" \"datasets>=2.14.0\" \"trl>=0.7.4\" \"accelerate>=0.24.0\" \"bitsandbytes>=0.41.0\" \"scipy>=1.11.0\" \"click>=8.0.0\" \"wandb>=0.15.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onAJMWGeB8b7"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "import torch\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Load model & tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"mistralai/Mistral-7B-v0.1\",\n",
        "    max_seq_length=2048,\n",
        "    full_finetuning=True\n",
        ")\n",
        "\n",
        "# (Keeping your current PEFT call; remove this line if you truly want full finetuning)\n",
        "# model = FastLanguageModel.get_peft_model(model)\n",
        "\n",
        "# Use a chat template that expects a list of {\"from\": ..., \"value\": ...}\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
        ")\n",
        "\n",
        "# Load dataset with 'prompt' and 'response' columns\n",
        "origdataset = load_dataset(\"alespalla/chatbot_instruction_prompts\", split=\"train\")\n",
        "pr_dataset = origdataset.select_columns([\"prompt\", \"response\"])\n",
        "\n",
        "# Convert each prompt/response pair into a 2-turn conversation, then to a single 'text' field\n",
        "def to_chat_text(batch):\n",
        "    conversations = []\n",
        "    for p, r in zip(batch[\"prompt\"], batch[\"response\"]):\n",
        "        conversations.append(\n",
        "            [\n",
        "                {\"from\": \"human\", \"value\": p},\n",
        "                {\"from\": \"gpt\", \"value\": r},\n",
        "            ]\n",
        "        )\n",
        "    texts = [\n",
        "        tokenizer.apply_chat_template(\n",
        "            conv, tokenize=False, add_generation_prompt=False\n",
        "        )\n",
        "        for conv in conversations\n",
        "    ]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = pr_dataset.map(\n",
        "    to_chat_text,\n",
        "    batched=True,\n",
        "    batch_size=100,\n",
        "    desc=\"Formatting prompt/response into chat template\",\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    dataset_num_proc=2,\n",
        "    max_seq_length=2048,\n",
        "    packing=False,  # Can make training faster for short sequences if set True\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=500,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bfloat16_supported(),\n",
        "        bf16=is_bfloat16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef3b46da"
      },
      "source": [
        "Now that the model is trained, you can use it to generate responses based on new prompts. The following cell shows an example of how to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ef752f71"
      },
      "outputs": [],
      "source": [
        "# Example inference\n",
        "prompt = \"Hi\" # Updated prompt\n",
        "\n",
        "# Apply the chat template to the prompt\n",
        "# We set add_generation_prompt=True to add the assistant's turn start token\n",
        "input_text = tokenizer.apply_chat_template([{\"from\": \"human\", \"value\": prompt}], tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "# Tokenize the input text and include attention mask\n",
        "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(model.device) # Added padding and truncation\n",
        "\n",
        "# Generate a response, passing the attention mask\n",
        "outputs = model.generate(inputs.input_ids, max_new_tokens=100, use_cache=True, attention_mask=inputs.attention_mask) # Added attention_mask\n",
        "\n",
        "# Decode the generated tokens back to text\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Print the original prompt and the generated response\n",
        "print(\"Prompt:\")\n",
        "print(prompt)\n",
        "print(\"\\nGenerated Response:\")\n",
        "print(response)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}