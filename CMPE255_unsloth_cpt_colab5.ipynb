{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "45a810ab",
      "metadata": {
        "id": "45a810ab"
      },
      "source": [
        "## 1) Install & restart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab196791",
      "metadata": {
        "id": "ab196791"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip -q install unsloth datasets accelerate bitsandbytes peft transformers trl huggingface_hub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b5b1e89",
      "metadata": {
        "id": "5b5b1e89"
      },
      "source": [
        "## 2) Imports, login & basic config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60b16d69",
      "metadata": {
        "id": "60b16d69"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from unsloth import (\n",
        "    FastLanguageModel,\n",
        "    UnslothTrainer,\n",
        "    UnslothTrainingArguments,\n",
        "    is_bfloat16_supported,\n",
        ")\n",
        "from huggingface_hub import login\n",
        "import torch, os, json\n",
        "\n",
        "# === Login to Hugging Face (paste your token when prompted) ===\n",
        "# If you've already saved it to Colab secrets or env, you can skip logging in again.\n",
        "try:\n",
        "    token = os.environ.get(\"HF_TOKEN\", None)\n",
        "    if token:\n",
        "        login(token=token, add_to_git_credential=True)\n",
        "    else:\n",
        "        login()  # will prompt in Colab\n",
        "except Exception as e:\n",
        "    print(\"Login skipped or failed:\", e)\n",
        "\n",
        "# === Config ===\n",
        "DATASET_NAME = \"Hindi-data-hub/odaigen_hindi_pre_trained_sp\"\n",
        "MODEL_NAME   = \"unsloth/llama-3-8b-bnb-4bit\"   # Use a BASE model (not Instruct) for CPT\n",
        "MAX_SEQ_LEN  = 2048\n",
        "LOAD_4BIT    = True\n",
        "DTYPE        = None  # auto-pick bf16/fp16\n",
        "OUTPUT_DIR   = \"cpt_lang_hi\"\n",
        "SEED         = 42\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ff72a2c",
      "metadata": {
        "id": "0ff72a2c"
      },
      "source": [
        "## 3) Load data set from Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f398d0b8",
      "metadata": {
        "id": "f398d0b8"
      },
      "outputs": [],
      "source": [
        "# Try loading the dataset. If access is restricted, make sure you've accepted conditions on the dataset page.\n",
        "# We'll first peek the dataset to discover column names and size.\n",
        "from datasets import get_dataset_config_names, get_dataset_split_names\n",
        "\n",
        "print(\"Checking dataset configs and splits...\")\n",
        "try:\n",
        "    configs = get_dataset_config_names(DATASET_NAME, token=True)\n",
        "except Exception as e:\n",
        "    print(\"Could not list configs (may be gated). Proceeding with default config. Error:\", e)\n",
        "    configs = [None]\n",
        "\n",
        "print(\"Configs:\", configs)\n",
        "\n",
        "split_names = []\n",
        "for cfg in configs:\n",
        "    try:\n",
        "        splits = get_dataset_split_names(DATASET_NAME, config_name=cfg, token=True)\n",
        "        split_names.append((cfg, splits))\n",
        "    except Exception as e:\n",
        "        split_names.append((cfg, [\"train\"]))\n",
        "print(\"Splits:\", split_names)\n",
        "\n",
        "# Load only 10% for training; use the next ~2% for eval if no validation split exists.\n",
        "try:\n",
        "    if configs and configs[0] is not None:\n",
        "        ds_train = load_dataset(DATASET_NAME, configs[0], split=\"train[:10%]\", token=True)\n",
        "    else:\n",
        "        ds_train = load_dataset(DATASET_NAME, split=\"train[:10%]\", token=True)\n",
        "except Exception as e:\n",
        "    print(\"Direct 'train[:10%]' split failed; trying explicit slicing fallback. Error:\", e)\n",
        "    if configs and configs[0] is not None:\n",
        "        ds_train = load_dataset(DATASET_NAME, configs[0], split=\"train[:10%]\", token=True)\n",
        "        ds_eval  = load_dataset(DATASET_NAME, configs[0], split=\"train[10%:12%]\", token=True)\n",
        "    else:\n",
        "        ds_train = load_dataset(DATASET_NAME, split=\"train[:10%]\", token=True)\n",
        "        ds_eval  = load_dataset(DATASET_NAME, split=\"train[10%:12%]\", token=True)\n",
        "else:\n",
        "    # Make eval split if not created above\n",
        "    try:\n",
        "        ds_eval = load_dataset(DATASET_NAME, split=\"validation\", token=True)\n",
        "    except Exception:\n",
        "        # If no validation split, carve ~2% out of the 10% train subset\n",
        "        ds_eval  = ds_train.shard(num_shards=50, index=0)  # ~2% of the 10%\n",
        "        ds_train = ds_train.shard(num_shards=50, index=1)\n",
        "\n",
        "print(ds_train)\n",
        "print(ds_eval)\n",
        "\n",
        "# Detect the primary text column\n",
        "text_column = None\n",
        "for cand in [\"text\", \"sentence\", \"content\", \"raw_text\", \"document\", \"data\"]:\n",
        "    if cand in ds_train.column_names:\n",
        "        text_column = cand\n",
        "        break\n",
        "\n",
        "if text_column is None:\n",
        "    # Heuristic: pick the first string column\n",
        "    for name in ds_train.column_names:\n",
        "        if isinstance(ds_train[0][name], str):\n",
        "            text_column = name\n",
        "            break\n",
        "\n",
        "if text_column is None:\n",
        "    raise ValueError(\"Could not find a text column. Please inspect ds_train.column_names and set one.\")\n",
        "\n",
        "print(\"Using text column:\", text_column)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8963bd5",
      "metadata": {
        "id": "b8963bd5"
      },
      "source": [
        "## 4) Tokenizer/model (4‑bit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d51bebe1",
      "metadata": {
        "id": "d51bebe1"
      },
      "outputs": [],
      "source": [
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=MODEL_NAME,\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        "    dtype=DTYPE,\n",
        "    load_in_4bit=LOAD_4BIT,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9827ef6",
      "metadata": {
        "id": "a9827ef6"
      },
      "source": [
        "## 5) Prepare tokenized data set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d52ec908",
      "metadata": {
        "id": "d52ec908"
      },
      "outputs": [],
      "source": [
        "\n",
        "def tok_fn(batch):\n",
        "    return tokenizer(\n",
        "        batch[text_column],\n",
        "        truncation=True,\n",
        "        max_length=MAX_SEQ_LEN,\n",
        "        return_attention_mask=False,\n",
        "    )\n",
        "\n",
        "tokenized_train = ds_train.map(tok_fn, batched=True, remove_columns=[c for c in ds_train.column_names if c != text_column])\n",
        "tokenized_eval  = ds_eval.map(tok_fn, batched=True, remove_columns=[c for c in ds_eval.column_names if c != text_column])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37c89efd",
      "metadata": {
        "id": "37c89efd"
      },
      "source": [
        "## 6) QLoRA (incl. embeddings & lm_head) and train (CPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d408e76",
      "metadata": {
        "id": "5d408e76"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\n",
        "        \"gate_proj\",\"up_proj\",\"down_proj\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "args = UnslothTrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=1,                 # increase for more data\n",
        "    per_device_train_batch_size=1,      # tune to your GPU\n",
        "    per_device_eval_batch_size=2,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=5e-5,                 # main LR for LoRA blocks\n",
        "    embedding_learning_rate=5e-6,       # smaller for embed/lm_head\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        "    gradient_checkpointing=True,\n",
        "    optim=\"adamw_8bit\",\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    logging_steps=50,\n",
        "    eval_strategy=\"steps\", # Changed from evaluation_strategy to eval_strategy\n",
        "    eval_steps=1000,\n",
        "    save_steps=1000,\n",
        "    save_total_limit=2,\n",
        "    max_grad_norm=1.0,\n",
        ")\n",
        "\n",
        "#To save memory\n",
        "model.config.use_cache = False\n",
        "\n",
        "trainer = UnslothTrainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_eval,\n",
        "    args=args,\n",
        "    tokenizer=tokenizer, # Explicitly pass the tokenizer\n",
        "    packing=True,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1aed180",
      "metadata": {
        "id": "d1aed180"
      },
      "source": [
        "## 7) Save LoRA and (optional) merged weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43638c23",
      "metadata": {
        "id": "43638c23"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "model.save_pretrained(f\"{OUTPUT_DIR}/lora\")\n",
        "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/lora\")\n",
        "\n",
        "# Merge LoRA into a single checkpoint (optional)\n",
        "model.save_pretrained(f\"{OUTPUT_DIR}/merged\", merge=True)\n",
        "tokenizer.save_pretrained(f\"{OUTPUT_DIR}/merged\")\n",
        "\n",
        "print(\"Saved LoRA to\", f\"{OUTPUT_DIR}/lora\")\n",
        "print(\"Saved merged to\", f\"{OUTPUT_DIR}/merged\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d87ce179",
      "metadata": {
        "id": "d87ce179"
      },
      "source": [
        "## 8) Quick perplexity & generation sanity check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0276b317",
      "metadata": {
        "id": "0276b317"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Use the trainer's built-in evaluate method which handles packing\n",
        "metrics = trainer.evaluate()\n",
        "pp = math.exp(metrics[\"eval_loss\"])\n",
        "print(\"Eval Perplexity:\", pp)\n",
        "\n",
        "# Simple generation test (raw text — no chat template during CPT)\n",
        "prompt = \"हिंदी में एक छोटा अनुच्छेद लिखें जो इस मॉडल की समझ का परीक्षण करे।\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "out = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=128,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        ")\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}