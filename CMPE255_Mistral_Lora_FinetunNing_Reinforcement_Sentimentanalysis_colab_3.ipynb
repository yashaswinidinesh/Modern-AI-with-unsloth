{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDV34W70B3Re"
      },
      "outputs": [],
      "source": [
        "!pip install -U unsloth[torch] trl transformers datasets accelerate peft bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onAJMWGeB8b7"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel, is_bfloat16_supported, PatchDPOTrainer\n",
        "from transformers import TrainingArguments\n",
        "from trl import DPOTrainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Patch TRL's DPO trainer with Unslothâ€™s speed-ups\n",
        "PatchDPOTrainer()  # important for Unsloth RL\n",
        "\n",
        "BASE_MODEL = \"mistralai/Mistral-7B-v0.1\"  # or another small instruct model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=BASE_MODEL,\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,            # memory saver; DPO + LoRA is fine here\n",
        ")\n",
        "model = FastLanguageModel.get_peft_model(model)  # add LoRA adapters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjsApjoPHIlg"
      },
      "outputs": [],
      "source": [
        "raw = load_dataset(\"trl-lib/lm-human-preferences-sentiment\")\n",
        "# DPO expects columns: prompt / chosen / rejected\n",
        "train = raw[\"train\"].select(range(3000))  # keep it small for Colab\n",
        "eval_ = raw[\"test\"].select(range(300))    # optional if present\n",
        "\n",
        "def keep_cols(x):\n",
        "    return {\"prompt\": x[\"prompt\"], \"chosen\": x[\"chosen\"], \"rejected\": x[\"rejected\"]}\n",
        "\n",
        "train = train.map(keep_cols, remove_columns=[c for c in train.column_names if c not in {\"prompt\",\"chosen\",\"rejected\"}])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXmZ_90xHXWC"
      },
      "outputs": [],
      "source": [
        "from unsloth import PatchDPOTrainer, is_bfloat16_supported\n",
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "# 0) Patch TRL's DPO with Unsloth speedups\n",
        "PatchDPOTrainer()\n",
        "\n",
        "# 1) Ensure pad token exists\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 2) Use DPOConfig (NOT TrainingArguments), include padding_value\n",
        "args = DPOConfig(\n",
        "    output_dir=\"dpo-rl-colab3\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=2e-5,\n",
        "    logging_steps=10,\n",
        "    save_steps=100,\n",
        "    num_train_epochs=1,                 # or use max_steps\n",
        "    bf16=is_bfloat16_supported(),\n",
        "    fp16=not is_bfloat16_supported(),\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    report_to=\"none\",\n",
        "    padding_value=tokenizer.pad_token_id,  # <-- important for Unsloth DPO\n",
        "    truncation_mode=\"keep_end\",            # safe default for the collator\n",
        ")\n",
        "\n",
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    ref_model=None,\n",
        "    beta=0.1,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=eval_ if \"test\" in raw else None,\n",
        "    tokenizer=tokenizer,\n",
        "    max_length=1024,\n",
        "    max_prompt_length=512,\n",
        "    args=args,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef3b46da"
      },
      "source": [
        "Now that the model is trained, you can use it to generate responses based on new prompts. The following cell shows an example of how to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "768858d1"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Create a pipeline for text generation\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Define a prompt relevant to the sentiment dataset\n",
        "prompt = \"This movie is amazing and I really loved it because\"\n",
        "\n",
        "# Define a prompt relevant to the sentiment dataset\n",
        "prompt = \"This movie is shit and I really disliked it because\"\n",
        "\n",
        "# Generate text\n",
        "generated_text = generator(prompt, max_length=50, num_return_sequences=1)[0]['generated_text']\n",
        "\n",
        "print(generated_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}